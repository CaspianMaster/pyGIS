{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54be2c25",
   "metadata": {},
   "source": [
    "(f_rs_ml_predict)=\n",
    "\n",
    "\n",
    "---------------\n",
    "```{admonition} Learning Objectives\n",
    "  - Fit and predict machine learning models to make spatial predictions\n",
    "  - Predict landcover or continuous models \n",
    "  - Make predictions using timeseries data\n",
    "\n",
    "```\n",
    "```{admonition} Review\n",
    "* [Geowombat IO](f_rs_io.md)\n",
    "* [Geowombat Extraction](f_rs_extraction.md)\n",
    "* [Sklearn_xarray](https://phausamann.github.io/sklearn-xarray/)\n",
    "* [Sklearn pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n",
    "```\n",
    "--------------\n",
    "\n",
    "\n",
    "# Spatial Prediction using ML in Python\n",
    "## Create Land Use Classification using Geowombat & Sklearn\n",
    "\n",
    "The most common task for remotely sensed data is creating land cover classification. In this tutorial we will walk you through how to train a ML model using raster data. These methods are heavily dependent on the great package [sklearn_xarray](https://phausamann.github.io/sklearn-xarray/). To understand the pipeline commands please see their documentation. \n",
    "\n",
    "In the following example we will use Landsat data, some training data to train a sklearn model. In order to do this we first need  to have land classifications for a set of points of polygons. In this case we have three polygons with the classes ['water','crop','tree','developed']. The first step is to use `LabelEncoder` to convert these to integer based categories, which we store in a new column called 'lc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geowombat as gw\n",
    "from geowombat.data import l8_224078_20200518, l8_224078_20200518_polygons\n",
    "\n",
    "from geowombat.ml import fit\n",
    "import geopandas as gpd\n",
    "from sklearn_xarray.preprocessing import Featurizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# The labels are string names, so here we convert them to integers\n",
    "labels = gpd.read_file(l8_224078_20200518_polygons)\n",
    "labels['lc'] = le.fit(labels.name).transform(labels.name)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4bf47",
   "metadata": {},
   "source": [
    "We are then going to generate our sklearn pipeline ([see simple tutorial here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)). A pipeline simply allows us to pass a numpy array through a defined set of operations. In this case the data is passed through the following operations:\n",
    "\n",
    " * `Featurizer`: [Stacks](https://phausamann.github.io/sklearn-xarray/content/api/preprocessing.html?highlight=featurizer#sklearn_xarray.preprocessing.Featurizer) a numpy array for use in sklearn\n",
    " * `StandardScaler`: [Normalizes](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) all variables by removing the mean and scaling to unit variance\n",
    " * `PCA`: Calculates [Principal Components](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA) to reduce dimensionality. \n",
    " * `GaussianNB`: Fits a [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB) model for a quick classification. \n",
    "\n",
    " In this example we will just fit a model. The trained pipeline can be accessed with the returned `clf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa534300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a data pipeline\n",
    "pl = Pipeline([('featurizer', Featurizer()),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('pca', PCA()),\n",
    "                ('clf', GaussianNB())])\n",
    "\n",
    "# Fit the classifier\n",
    "with gw.config.update(ref_res=100):\n",
    "    with gw.open(l8_224078_20200518, chunks=128) as src:\n",
    "        X, clf = fit(src, labels, pl, col='lc')\n",
    "\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea526db",
   "metadata": {},
   "source": [
    "In order to fit and predict to our original data we simply use `fit_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5, 5)) \n",
    "fig, ax = plt.subplots(dpi=200,figsize=(10,10))\n",
    "\n",
    "from geowombat.ml import fit_predict\n",
    "\n",
    "with gw.config.update(ref_res=100):\n",
    "    with gw.open(l8_224078_20200518, chunks=128) as src:\n",
    "        y = fit_predict(src, labels, pl, col='lc')\n",
    "        print(y)\n",
    "        y.plot(robust=True, ax=ax)\n",
    "plt.tight_layout(pad=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be573855",
   "metadata": {},
   "source": [
    "## Spatial prediction with time series stack using Geowombat & Sklearn\n",
    "\n",
    "If you have a stack of time series data it is simple to apply the same method as we described previously, except we need to open multiple images, set `stack_dim` to 'time' and set the `time_names`.  *Note* we are just pretending we have two dates of LandSat imagery here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gw.config.update(ref_res=100):\n",
    "   with gw.open([l8_224078_20200518, l8_224078_20200518], time_names=['t1', 't2'], stack_dim='time', chunks=128) as src:\n",
    "        y = fit_predict(src, labels, pl, col='lc')\n",
    "        print(y)"
   ]
  }
 ],
 "metadata": {
  "html_meta": {
   "description lang=en": "Spatial classification and prediction models. Train and fit sklearn models on raster data including LandSat or other gridded data. ",
   "description lang=es": "Modelos de clasificación y predicción espacial. Entrene y ajuste modelos sklearn en datos ráster, incluido LandSat u otros datos cuadriculados.",
   "description lang=fr": "Classification spatiale et modèles de prédiction. Entraînez et adaptez des modèles sklearn sur des données raster, y compris LandSat ou d'autres données maillées.",
   "keywords": "sklearn, geospatial,raster, remote sensing, time series, landsat, sentinel",
   "property=og:locale": "en_US"
  },
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   16,
   44,
   63,
   73,
   87,
   90,
   104,
   110
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}